{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models in PyTorch\n",
    "\n",
    "\n",
    "## `torch.nn.Module` and `torch.nn.Parameter`\n",
    "\n",
    "In this video, we'll be discussing some of the tools PyTorch makes available for building deep learning networks.\n",
    "\n",
    "Except for `Parameter`, the classes we discuss in this video are all subclasses of `torch.nn.Module`. This is the PyTorch base class meant to encapsulate behaviors specific to PyTorch Models and their components.\n",
    "\n",
    "One important behavior of `torch.nn.Module` is registering parameters. If a particular `Module` subclass has learning weights, these weights are expressed as instances of `torch.nn.Parameter`. The `Parameter` class is a subclass of `torch.Tensor`, with the special behavior that when they are assigned as attributes of a `Module`, they are added to the list of that modules parameters. These parameters may be accessed through the `parameters()` method on the `Module` class.\n",
    "\n",
    "As a simple example, here's a very simple model with two linear layers and an activation function. We'll create an instance of it and ask it to report on its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinayModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      " just one layer\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      " model params\n",
      "Parameter containing:\n",
      "tensor([[-0.0988,  0.0512, -0.0066,  ..., -0.0746, -0.0875, -0.0018],\n",
      "        [-0.0030, -0.0087, -0.0874,  ...,  0.0037,  0.0392, -0.0812],\n",
      "        [-0.0033, -0.0378,  0.0218,  ..., -0.0182,  0.0854, -0.0302],\n",
      "        ...,\n",
      "        [-0.0351,  0.0166,  0.0767,  ..., -0.0333,  0.0219, -0.0087],\n",
      "        [ 0.0162, -0.0273,  0.0085,  ...,  0.0931, -0.0883, -0.0425],\n",
      "        [-0.0534, -0.0935,  0.0824,  ...,  0.0852, -0.0774, -0.0522]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0458, -0.0609, -0.0246, -0.0514, -0.0832, -0.0808, -0.0137, -0.0693,\n",
      "        -0.0593,  0.0447, -0.0474,  0.0415, -0.0423, -0.0708, -0.0860, -0.0606,\n",
      "        -0.0798,  0.0401, -0.0384, -0.0144,  0.0608,  0.0862, -0.0015, -0.0560,\n",
      "         0.0871, -0.0115,  0.0200, -0.0722, -0.0515, -0.0618, -0.0365, -0.0242,\n",
      "        -0.0368,  0.0767,  0.0812,  0.0111, -0.0940,  0.0066, -0.0668, -0.0867,\n",
      "        -0.0491, -0.0080,  0.0816,  0.0924, -0.0548,  0.0275,  0.0955, -0.0565,\n",
      "        -0.0688,  0.0944,  0.0262,  0.0051, -0.0520,  0.0502,  0.0089, -0.0088,\n",
      "         0.0535, -0.0704,  0.0590, -0.0491, -0.0343,  0.0107, -0.0914, -0.0668,\n",
      "         0.0043,  0.0242,  0.0876, -0.0492, -0.0450,  0.0139,  0.0501, -0.0939,\n",
      "        -0.0221,  0.0504,  0.0562, -0.0664,  0.0966,  0.0250, -0.0567,  0.0084,\n",
      "         0.0155, -0.0977, -0.0766,  0.0640, -0.0078, -0.0886, -0.0862, -0.0957,\n",
      "        -0.0021,  0.0980,  0.0810,  0.0544, -0.0353, -0.0440,  0.0098,  0.0667,\n",
      "         0.0254, -0.0160,  0.0259,  0.0495,  0.0324, -0.0906, -0.0018,  0.0663,\n",
      "        -0.0426, -0.0897, -0.0324, -0.0174, -0.0898,  0.0685, -0.0711, -0.0208,\n",
      "         0.0197,  0.0760, -0.0441,  0.0100, -0.0635,  0.0695,  0.0134, -0.0106,\n",
      "         0.0205,  0.0687, -0.0952, -0.0501,  0.0214, -0.0907, -0.0808,  0.0079,\n",
      "         0.0213, -0.0815, -0.0270, -0.0671, -0.0718,  0.0134, -0.0251, -0.0813,\n",
      "         0.0748, -0.0832, -0.0905,  0.0376,  0.0282,  0.0103,  0.0558,  0.0161,\n",
      "         0.0089,  0.0655, -0.0314, -0.0972,  0.0572, -0.0632, -0.0102,  0.0484,\n",
      "         0.0334,  0.0223, -0.0105, -0.0070,  0.0668, -0.0723, -0.0239,  0.0953,\n",
      "         0.0704,  0.0886,  0.0236, -0.0301,  0.0247, -0.0554,  0.0305,  0.0672,\n",
      "        -0.0382,  0.0475, -0.0025, -0.0407, -0.0627,  0.0867, -0.0112, -0.0150,\n",
      "         0.0281,  0.0961,  0.0910, -0.0224,  0.0536, -0.0886,  0.0957,  0.0606,\n",
      "         0.0059,  0.0129,  0.0716,  0.0079,  0.0188, -0.0821,  0.0656, -0.0650,\n",
      "         0.0106,  0.0439, -0.0057, -0.0911,  0.0764,  0.0934,  0.0284, -0.0815],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0265,  0.0646,  0.0529,  ..., -0.0337, -0.0119,  0.0337],\n",
      "        [-0.0521,  0.0182,  0.0669,  ...,  0.0599,  0.0204,  0.0060],\n",
      "        [ 0.0032,  0.0095, -0.0193,  ...,  0.0509, -0.0601,  0.0379],\n",
      "        ...,\n",
      "        [ 0.0227, -0.0582, -0.0522,  ...,  0.0367, -0.0129,  0.0377],\n",
      "        [-0.0438, -0.0252,  0.0494,  ..., -0.0157,  0.0550, -0.0140],\n",
      "        [ 0.0654, -0.0473, -0.0081,  ..., -0.0458, -0.0575, -0.0313]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0184, -0.0609,  0.0235,  0.0510,  0.0550,  0.0130,  0.0681,  0.0083,\n",
      "         0.0512,  0.0225], requires_grad=True)\n",
      "\n",
      "\n",
      " layer param\n",
      "Parameter containing:\n",
      "tensor([[-0.0265,  0.0646,  0.0529,  ..., -0.0337, -0.0119,  0.0337],\n",
      "        [-0.0521,  0.0182,  0.0669,  ...,  0.0599,  0.0204,  0.0060],\n",
      "        [ 0.0032,  0.0095, -0.0193,  ...,  0.0509, -0.0601,  0.0379],\n",
      "        ...,\n",
      "        [ 0.0227, -0.0582, -0.0522,  ...,  0.0367, -0.0129,  0.0377],\n",
      "        [-0.0438, -0.0252,  0.0494,  ..., -0.0157,  0.0550, -0.0140],\n",
      "        [ 0.0654, -0.0473, -0.0081,  ..., -0.0458, -0.0575, -0.0313]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0184, -0.0609,  0.0235,  0.0510,  0.0550,  0.0130,  0.0681,  0.0083,\n",
      "         0.0512,  0.0225], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinayModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinayModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100,200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "tinaymodel = TinayModel()\n",
    "\n",
    "print(\"The model:\")\n",
    "print(tinaymodel)\n",
    "\n",
    "print('\\n\\n just one layer')\n",
    "print(tinaymodel.linear2)\n",
    "\n",
    "print('\\n\\n model params')\n",
    "for param in tinaymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\n layer param')\n",
    "for param in tinaymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the fundamental structure of a PyTorch model: there is an `__init__()` method that defines the layers and other components of a model, and a `forward()` method where the computation gets done. Note that we can print the model, or any of its submodules, to learn about its structure.\n",
    "\n",
    "## Common Layer Types\n",
    "\n",
    "### Linear Layers\n",
    "\n",
    "The most basic type of neural network layer is a *linear* or *fully connected* layer. This is a layer where every input influences every output of the layer to a degree specified by the layer's weights. If a model has *m* inputs and *n* outputs, the weights will be an *m * n* matrix. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "tensor([[0.6882, 0.0604, 0.1644]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters\n",
      "Parameter containing:\n",
      "tensor([[ 0.2786,  0.0088, -0.0618],\n",
      "        [ 0.3670,  0.2826, -0.0133]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.5670, 0.0316], requires_grad=True)\n",
      "\n",
      "\n",
      "output\n",
      "tensor([[0.7491, 0.2990]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin  = torch.nn.Linear(3,2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\noutput')\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do the matrix multiplication of `x` by the linear layer's weights, and add the biases, you'll find that you get the output vector `y`.\n",
    "\n",
    "One other important feature to note: When we checked the weights of our layer with `lin.weight`, it reported itself as a `Parameter` (which is a subclass of `Tensor`), and let us know that it's tracking gradients with autograd. This is a default behavior for `Parameter` that differs from `Tensor`.\n",
    "\n",
    "Linear layers are used widely in deep learning models. One of the most common places you'll see them is in classifier models, which will usually have one or more linear layers at the end, where the last layer will have *n* outputs, where *n* is the number of classes the classifier addresses.\n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "*Convolutional* layers are built to handle data with a high degree of spatial correlation. They are very commonly used in computer vision, where they detect close groupings of features which the compose into higher-level features. They pop up in other contexts too - for example, in NLP applications, where the a word's immediate context (that is, the other words nearby in the sequence) can affect the meaning of a sentence.\n",
    "\n",
    "We saw convolutional layers in action in LeNet5 in an earlier video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = wx + b\n",
    "        self.fc1 = torch.nn.Linear(16, 6, 6, 120)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max poling over (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # if the size is square you can only specify a single number only\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] #all dimention except the batch dimention\n",
    "        num_feature = 1\n",
    "        for s in size:\n",
    "            num_feature *= s \n",
    "        return num_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what's happening in the convolutional layers of this model. Starting with `conv1`:\n",
    "\n",
    "* LeNet5 is meant to take in a 1x32x32 black & white image. **The first argument to a convolutional layer's constructor is the number of input channels.** Here, it is 1. If we were building this model to look at 3-color channels, it would be 3.\n",
    "* A convolutional layer is like a window that scans over the image, looking for a pattern it recognizes. These patterns are called *features,* and one of the parameters of a convolutional layer is the number of features we would like it to learn. **This is the second argument to the constructor is the number of output features.** Here, we're asking our layer to learn 6 features.\n",
    "* Just above, I likened the convolutional layer to a window - but how big is the window? **The third argument is the window or *kernel* size.** Here, the \"5\" means we've chosen a 5x5 kernel. (If you want a kernel with height different from width, you can specify a tuple for this argument - e.g., `(3, 5)` to get a 3x5 convolution kernel.)\n",
    "\n",
    "The output of a convolutional layer is an *activation map* - a spatial representation of the presence of features in the input tensor. `conv1` will give us an output tensor of 6x28x28; 6 is the number of features, and 28 is the height and width of our map. (The 28 comes from the fact that when scanning a 5-pixel window over a 32-pixel row, there are only 28 valid positions.)\n",
    "\n",
    "We then pass the output of the convolution through a ReLU activation function (more on activation functions later), then through a max pooling layer. The max pooling layer takes features near each other in the activation map and groups them together. It does this by reducing the tensor, merging every 2x2 group of cells in the output into a single cell, and assigning that cell the maximum value of the 4 cells that went into it. This gives us a lower-resolution version of the activation map, with dimensions 6x14x14.\n",
    "\n",
    "Our next convolutional layer, `conv2`, expects 6 input channels (corresponding to the 6 features sought by the first layer), has 16 output channels, and a 3x3 kernel. It puts out a 16x12x12 activation map, which is again reduced by a max pooling layer to 16x6x6. Prior to passing this output to the linear layers, it is reshaped to a 16 * 6 * 6 = 576-element vector for consumption by the next layer.\n",
    "\n",
    "There are convolutional layers for addressing 1D, 2D, and 3D tensors. There are also many more optional arguments for a conv layer constructor, including stride length(e.g., only scanning every second or every third position) in the input, padding (so you can scan out to the edges of the input), and more. See the [documentation](https://pytorch.org/docs/stable/nn.html#convolution-layers) for more information.\n",
    "\n",
    "### Recurrent Layers\n",
    "\n",
    "*Recurrent neural networks* (or *RNNs)* are used for sequential data - anything from time-series measurements from a scientific instrument to natural language sentences to DNA nucleotides. An RNN does this by maintaining a *hidden state* that acts as a sort of memory for what it has seen in the sequence so far.\n",
    "\n",
    "The internal structure of an RNN layer - or its variants, the LSTM (long short-term memory) and GRU (gated recurrent unit) - is moderately complex and beyond the scope of this video, but we'll show you what one looks like in action with an LSTM-based part-of-speech tagger (a type of classifier that tells you if a word is a noun, verb, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
